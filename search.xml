<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[s3fs使用指南]]></title>
      <url>%2F2016%2F12%2F06%2Fs3fs-guide%2F</url>
      <content type="text"><![CDATA[概述KS3（Kingsoft Standard Storage Service）服务是一个对象存储类服务。类似于网盘，可以通过网络随时随地的获取存储数据。金山云为用户管理KS3存储桶提供了两种控制管理方式：金山云的后台Consol、金山云官方KS3的SDK。 相比于以上两种使用场景，在实际的开发应用中，常常还有这样的场景：把KS3存储桶mount到本地文件系统，使得用户可以像操作文件目录一样操作KS3的对象。 本节所介绍的S3FS fuse，就是这么一款工具；针对linux操作系统，帮助用户把KS3的桶mount到本地文件系统，使得用户操作存储桶里的数据就如同操作本地文件一样方便。 安装依赖环境准备Ubuntu环境1sudo apt-get install automake autotools-dev g++ git libcurl4-gnutls-dev libfuse-dev libssl-dev libxml2-dev make pkg-config CentOS环境1sudo yum install automake fuse fuse-devel gcc-c++ git libcurl-devel libxml2-devel make openssl-devel 源码安装下载源码1git clone https://github.com/s3fs-fuse/s3fs-fuse.git 安装12345cd s3fs-fuse./autogen.sh./configuremakesudo make install 使用配置KS3秘钥配置KS3的KS3ACCESSKEYID、KS3SECRETACCESSKEY信息，并将其存储在/etc/passwd-s3fs 文件中1echo KS3ACCESSKEYID:KS3SECRETACCESSKEY &gt; /etc/passwd-s3fs 修改秘钥文件权限，来限制访问：1chmod 600 /etc/passwd-s3fs 挂载KS3存储桶将KS3 bucket 挂载到指定目录1s3fs &lt;your-ks3-bucket-name&gt; &lt;your-mount-point&gt; -o url=&lt;your-ks3-endpoint&gt; 示例：将KS3北京region的 ks3-fs-test 存储桶挂载到本地 /mnt/ks3 目录下:(Ks3AccesskeyId: sgyO124X+/ZRbmgrbP7d， Ks3SecretAccessKey:p9/6yLzbU2FZEPw5vCPf2/buQOz1SVNdqqBtzscH， ks3 endpoint: http://ks3-cn-shanghai-internal.ksyun.com)1234echo sgyO124X+/ZRbmgrbP7d:p9/6yLzbU2FZEPw5vCPf2/buQOz1SVNdqqBtzscH &gt; /etc/passwd-s3fschmod 600 /etc/passwd-s3fsmkdir /mnt/ks3s3fs ks3-fs-test /mnt/ks3 -o passwd_file=/etc/passwd-s3fs -o url=http://ks3-cn-shanghai-internal.ksyun.com 多线程分块上传1s3fs &lt;your-ks3-bucket-name&gt; &lt;your-mount-point&gt; -o multipart_size=100 -o parallel_count=10 -o sigv2 -o curldbg -o url=&lt;your-ks3-endpoint&gt; 注:-o multipart_size 100: 分块大小100M-o parallel_count 10 : 10线程并发 卸载bucket:fusermount -u /mnt/s3fs 功能支持 文件操作 测试结果 备注 mkdir pass listdir pass rmdir pass isfile pass isdir pass isabs pass exists pass makedirs pass stat pass getsize pass mknod pass open pass read pass readline pass readlines pass close pass flush pass fileno pass lell pass next pass seek pass truncate pass rename pass copyfile pass move pass remove pass rmdir pass copy pass copytree pass chmod Filed 局限性s3fs提供的功能和性能和本地文件系统相比，具有一些局限性。具体包括： 随机或追加写文件会导致整个文件的重写。 元数据操作，例如list directory，性能较差，因为需要远程访问KS3服务器。 最终一致性可能导致临时过期的数据。 文件/文件夹的重命名操作不是原子的。 不支持hard link。 不适合用在高并发读/写的场景，这样会让系统的load升高。 多个客户端挂载同一个KS3 bucket时，依赖用户自行协调各个客户端的行为。例如避免多个客户端写同一个文件等等。 使用场景FAQ1 Q: s3fs适合什么样的程序？s3fs能把ks3 bucket挂载到本地，如果您使用的软件没有支持KS3，但您又想让数据能自动同步到KS3，那么s3fs是一个很好的选择。 2 Q: s3fs一定要金山云的机器才能用么？s3fs不限制一定要金山云的内网才可以使用，外网机器依然可以使用。只是内网机房有KS3的专线，保证了KS3的访问速度。 3 Q: s3fs如何Debug？您可以使用在挂载时，加上-d -o f2参数，s3fs会把日志写入到系统日志中。在centos系统中，在/var/log/messages中。您也可以在挂载时使用-f -d -o f2参数，s3fs会把日志输出到屏幕上。 4 Q: s3fs提示s3fs: unable to access MOUNTPOINT /mnt/ks3: No such file or directory这是您未创建该目录导致的，在挂载前需要创建对应目录。 5 Q: 如何使用supervisor启动s3fs？ 安装supervisor，在ubuntu中执行sudo apt-get install supervisor 建立一个目录，编辑s3fs的启动脚本： 1234mkdir /root/s3fs_scriptsvi /root/s3fs_scripts/start_s3fs.sh# contents of start_s3fs.shs3fs ks3test /mnt/ks3 -f -o passwd_file=/home/lain/passwd-ks3fs -o multipart_size=100 -o parallel_count=10 -o sigv2 -o curldbg -o url=http://ks3-cn-shanghai-internal.ksyun.com 编辑/etc/supervisor/supervisord.conf，在最后加入下面一段： 1234567[program:s3fs]command=bash /root/s3fs_scripts/start_s3fs.shlogfile=/var/log/s3fs.loglog_stdout=truelog_stderr=truelogfile_maxbytes=1MBlogfile_backups=10 运行supervisor： 1supervisord 确认一切正常： 1234ps aux | grep supervisor # 应该能看到supervisor进程ps aux | grep s3fs # 应该能看到s3fs进程killall s3fs # 杀掉s3fs进程，supervisor应该会重启它ps aux | grep s3fs # 应该能看到s3fs进程 如果出错，请检查/var/log/supervisor/supervisord.log和/var/log/s3fs.log。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Rclone使用指南]]></title>
      <url>%2F2016%2F11%2F28%2Frclone-guide%2F</url>
      <content type="text"><![CDATA[概述Rclone是一款的命令行工具，支持在不同对象存储、网盘间同步、上传、下载数据。 支持的主流对象存储： Google DriveAmazon S3Openstack Swift / Rackspace cloud files / Memset MemstoreDropboxGoogle Cloud StorageAmazon DriveMicrosoft One DriveHubicBackblaze B2Yandex DiskThe local filesystem 安装Linux下安装下载安装包123curl -O http://downloads.rclone.org/rclone-current-linux-amd64.zipunzip rclone-current-linux-amd64.zipcd rclone-*-linux-amd64 安装123sudo cp rclone /usr/sbin/sudo chown root:root /usr/sbin/rclonesudo chmod 755 /usr/sbin/rclone 配置1rclone MacOS下安装下载安装包12cd &amp;&amp; curl -O http://downloads.rclone.org/rclone-current-osx-amd64.zipunzip -a rclone-current-osx-amd64.zip &amp;&amp; cd rclone-*-osx-amd64 安装12sudo mv rclone /usr/local/bin/cd .. &amp;&amp; rm -rf rclone-*-osx-amd64 rclone-current-osx-amd64.zip 配置1rclone config 使用操作命令rclone命令的语法格式：1Syntax: [options] subcommand &lt;parameters&gt; &lt;parameters...&gt; 常用的rclone命令有： rclone config - 以控制会话的形式添加rclone的配置，配置保存在.rclone.conf文件中。 rclone copy - 将文件从源复制到目的地址，跳过已复制完成的。 rclone sync - 将源数据同步到目的地址，只更新目的地址的数据。 rclone move - 将源数据移动到目的地址。 rclone delete - 删除指定路径下的文件内容。 rclone purge - 清空指定路径下所有文件数据。 rclone mkdir - 创建一个新目录。 rclone rmdir - 删除空目录。 rclone check - 检查源和目的地址数据是否匹配。 rclone ls - 列出指定路径下所有的文件以及文件大小和路径。 rclone lsd - 列出指定路径下所有的目录/容器/桶。 rclone lsl - 列出指定路径下所有文件以及修改时间、文件大小和路径。 rclone md5sum - 为指定路径下的所有文件产生一个md5sum文件。 rclone sha1sum - 为指定路径下的所有文件产生一个sha1sum文件。 rclone size - 获取指定路径下，文件内容的总大小。. rclone version - 查看当前版本。 rclone cleanup - 清空remote。 rclone dedupe - 交互式查找重复文件，进行删除/重命名操作。 rclone config开启一个交互式的配置会话。命令格式如下：1rclone config rclone copy将文件从源复制到目的地址，跳过已复制完成的。命令格式如下：1rclone copy source:sourcepath dest:destpsth 注：1、rclone copy复制总是指定路径下的数据；而不是当前目录。2、–no-traverse 标志用于控制是否列出目的地址目录。 rclone sync1rclone sync source:path dest:path 注：1、同步数据时，可能会删除目的地址的数据；建议先使用–dry-run标志来检查要复制、删除的数据。2、同步数据出错时，不会删除任何目的地址的数据。3、rclone sync同步的始终是path目录下的数据，而不是path目录。（空目录将不会被同步） rclone move1rclone move source:path dest:path 注：1、同步数据时，可能会删除目的地址的数据；建议先使用–dry-run标志来检查要复制、删除的数据。 rclone purge清空path目录和数据。命令格式如下：1rclone purge remote:path 注：1、此命令，include/exclude 过滤器失效。2、删除path目录下部分数据，请使用rclone delete 命令 rclone mkdir创建path目录。命令格式如下：1rclone mkdir remote:path rclone rmdir删除一个空目录。命令格式如下：1rclone rmdir remote:path 注：1、不能删除非空的目录，删除非空目录请使用 rclone purge。 rclone check检查源和目标地址文件是否匹配。命令格式如下：1rclone check source:path dest:path 注：1、–size-only标志用于指定，只比较大小，不比较MD5SUMs。 rclone ls列出指定path下，所有的文件以及文件大小和路径。命令格式如下：1rclone ls remote:path rclone lsd列出指定path下，所有目录、容器、桶。命令格式如下：1rclone lsd remote:path rclone delete删除指定目录的内容。命令格式如下：1rclone delete remote:path 注：1、不同于rclone purge，rclone delete 可使用 include/exclude 过滤器选择删除文件内容。 eg： 删除文件大小大于100M的文件123456# 先检查哪些文件将被删除rclone --min-size 100M lsl remote:path # 使用rclone lsl 列出大于100M的文件rclone --dry-run --min-size 100M delete remote:path # 使用--dry-run 检查将要被删除的文件 # 使用 rclone delete 进行文件删除rclone --min-size 100M delete remote:path rclone size获取指定path下所有数据文件的总大小。命令格式如下：1rclone size remote:path more更多rclone命令，详见http://rclone.org/commands/ 。 配置rclonerclone 提供了交互式配置会话的方式来进行rclone的配置；每次配置的信息存储在.rclone.conf文件中。 配置示例在这里，我们使用共享存储KS3示例，来进行rclone的配置： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116# 执行rclone config 开启配置会话$ rclone config No remotes found - make a new onen) New remotes) Set configuration passwordq) Quit confign/s/q&gt; n # 输入n，配置一个新的remotename&gt; ks3-remote # 输入remote的名字，这里命名为 ks3-remoteType of storage to configure.Choose a number from below, or type in your own value 1 / Amazon Drive \ "amazon cloud drive" 2 / Amazon S3 (also Dreamhost, Ceph, Minio) \ "s3" 3 / Backblaze B2 \ "b2" 4 / Dropbox \ "dropbox" 5 / Encrypt/Decrypt a remote \ "crypt" 6 / Google Cloud Storage (this is not Google Drive) \ "google cloud storage" 7 / Google Drive \ "drive" 8 / Hubic \ "hubic" 9 / Kingsoft Cloud KS3 \ "ks3"10 / Local Disk \ "local"11 / Microsoft OneDrive \ "onedrive"12 / Openstack Swift (Rackspace Cloud Files, Memset Memstore, OVH) \ "swift"13 / Yandex Disk \ "yandex"Storage&gt; 9 # 选择9，也可直接输入ks3Get KS3 credentials from runtime (environment variables). Only applies if access_key_id and secret_access_key is blank.Choose a number from below, or type in your own value 1 / Enter KS3 credentials in the next step \ "false" 2 / Get KS3 credentials from the environment (env vars) \ "true"env_auth&gt; 1 # 选择1，直接配置access_key_id 和 secret_access_key AWS Access Key ID - leave blank for anonymous access or runtime credentials.access_key_id&gt; sgyO124X+/ZRbmgrbP7d # 键入ks3的access_key_idAWS Secret Access Key (password) - leave blank for anonymous access or runtime credentials.secret_access_key&gt; p9/6yLzbU2FZEPw5vCPf2/buQOz1SVNdqqBtzscH # 键入ks3的secret_access_keyRegion to connect to.Choose a number from below, or type in your own value 1 / Beijing Region(the default region). \ "ks3-cn-beijing" 2 / Hangzhou Region. \ "ks3-cn-hangzhou" 3 / Shanghai Region. \ "ks3-cn-shanghai" 4 / HongKong Region. \ "ks3-cn-hk-1" 5 / US West Region. \ "ks3-us-west-1"region&gt; 1 # 选择ks3的region，这里选择北京的regionEndpoint for KS3 API.Leave blank if using KS3 to use the default endpoint for the region.endpoint&gt; # 不使用外部的endpoint，直接回车略过An internal endpoint or the public endpoint for KS3 access. The default is false.Choose a number from below, or type in your own value 1 / Internal endpoint. \ "false" 2 / Public endpoint. \ "true"internal&gt; 1 # 选择1，从公网访问，不使用内网加速。 在金山云的云主机中建议选择2，使用内网加速，且不计流量。Canned ACL used when creating buckets and/or storing objects in KS3.Choose a number from below, or type in your own value 1 / Owner gets FULL_CONTROL. No one else has access rights (default). \ "private" 2 / Owner gets FULL_CONTROL. The AllUsers group gets READ access. \ "public-read" / Owner gets FULL_CONTROL. The AllUsers group gets READ and WRITE access. 3 | Granting this on a bucket is generally not recommended. \ "public-read-write"acl&gt; 1 # 选择1（私有控制）The server-side encryption algorithm used when storing this object in S3.Choose a number from below, or type in your own value 1 / None \ "" 2 / AES256 \ "AES256"server_side_encryption&gt; 2 # 选择2，使用AES256加密。Remote config--------------------[ks3-remote]env_auth = falseaccess_key_id = sgyO124X+/ZRbmgrbP7dsecret_access_key = p9/6yLzbU2FZEPw5vCPf2/buQOz1SVNdqqBtzscHregion = ks3-cn-beijingendpoint = internal = falseacl = privateserver_side_encryption = AES256--------------------y) Yes this is OKe) Edit this remoted) Delete this remotey/e/d&gt; y # 检查生成的配置，确认无误，选择y确认Current remotes:Name Type==== ====ks3-remote ks3e) Edit existing remoten) New remoted) Delete remotes) Set configuration passwordq) Quit confige/n/d/s/q&gt; q # 配置完成，选择q退出。 配置完成，检查生成的配置文件~/.rclone.conf:1234567891011$ more ~/.rclone.conf[ks3-remote] # 远程存储的名称：ks3-remotetype = ks3 # 远程存储的类型：ks3env_auth = false # 是否从环境变量中读取access_key和secret_key access_key_id = sgyO124X+/ZRbmgrbP7d # 当前ks3的access_keysecret_access_key = p9/6yLzbU2FZEPw5vCPf2/buQOz1SVNdqqBtzscH # 当前ks3的secret_keyregion = ks3-cn-beijing # 所使用ks3的region名称endpoint = # 是否使用外部endpointinternal = false # 是否从内网访问acl = private # 访问控制策略：私有控制server_side_encryption = AES256 # 加密 测试配置查看上述配置远程存储所有的桶：123456789$ rclone lsd ks3-remote: -1 2016-06-07 07:26:04 -1 dockertest2 -1 2016-11-21 08:24:52 -1 ks3-fs-test2016/11/28 15:26:25 Transferred: 0 Bytes (0 Bytes/s)Errors: 0Checks: 0Transferred: 0Elapsed time: 200ms 一些实用场景将本地目录同步到ks3一般，由于对象存储具有廉价、高可用、安全、可靠等特性；我们通常会用作本地数据（日志、图片、视频）的存储或是备份。通常在使用对象存储存储数据时，往往需要调用官方的SDK，需要一定的开发量。尤其当我们仅仅是需要备份某个盘或是目录下的日志时，这时候使用rclone这样的命令行工具，能方便把数据存储（或备份）到远程存储上，且不受文件大小的限制（比如ks3单个文件上传大小不能超过5G）。 比如，将/home/local/directory同步到远程存储的某个bucket中，操作命令如下：1$ rclone sync /home/local/directory remote:bucket AWS s3 迁移到 KS3由于s3的region多建在国外，s3的资费相对较高。那么将一部分数据迁移到KS3，是比较经济的。除了使用KS3官方提供的镜像服务，可以别的远程存储迁移到KS3；rclone为此提供了另外一种可能。 比如将远程存储s3的桶s3-log，迁移到远程存储ks3的桶ks3-log:1$ rclone sync s3-remote:s3-log ks3-remote:ks3-log 远程存储s3-remote、ks3-remote的配置信息如下：12345678910111213141516171819202122[s3-remote]type = s3-remoteenv_auth = falseaccess_key_id = AKIAJTT3FZHMRQSXFZFQsecret_access_key = pfhw/eMEpU4vMDCHJBuPM8SMgLnguncZdtfikq4uregion = ap-northeast-2endpoint = location_constraint = ap-northeast-2acl = privateserver_side_encryption = AES256storage_class = [ks3-remote]type = ks3env_auth = falseaccess_key_id = sgyO124X+/ZRbmgrbP7dsecret_access_key = p9/6yLzbU2FZEPw5vCPf2/buQOz1SVNdqqBtzscHregion = ks3-cn-beijingendpoint = internal = falseacl = privateserver_side_encryption = AES256 KS3 bucket之间的同步KS3对象存储不支持使用KS3存储的桶作为另一个桶的镜像。 比如：我们需要搭建一个大型的docker hub，为了保证不同地域用户下载和上传速度（虽然KS3自带CDN），需要在北京、上海分别建立两个镜像中心；这样在拉取镜像时，南方用户只需要去上海的镜像中心拉取数据，北方用户则到北京镜像中心拉取数据；极大地提高数据的访问速度。那么类似这样的需求，我们不得不保持两个镜像中心的数据一致性；在不适用官方SDK的情况下，我们可以使用rclone sync同步不同region下bucket中的数据：1$ rclone sync ks3-beijing:bucket1 ks3-shanghai:bucket2 注：即便bucket1与bucket2共用一套账户体系，即ks3-beijing 和 ks3-shanghai的access_key_id和secret_access_key是一样的。仍然需要分别建立 ks3-beijing 和 ks3-shanghai 两个远程存储的配置。例如：1234567891011121314151617181920212223$ more ~/.rclone[ks3-beijing] type = ks3 env_auth = falseaccess_key_id = sgyO124X+/ZRbmgrbP7dsecret_access_key = p9/6yLzbU2FZEPw5vCPf2/buQOz1SVNdqqBtzscHregion = ks3-cn-beijingendpoint = internal = trueacl = privateserver_side_encryption = AES256 [ks3-shanghai] type = ks3 env_auth = falseaccess_key_id = sgyO124X+/ZRbmgrbP7dsecret_access_key = p9/6yLzbU2FZEPw5vCPf2/buQOz1SVNdqqBtzscHregion = ks3-cn-shanghaiendpoint = internal = trueacl = privateserver_side_encryption = AES256]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[基于Jenkins、Gitlab构建持续集成、持续交付的Docker容器集群]]></title>
      <url>%2F2016%2F11%2F14%2Fjenkins-gitlab-docker%2F</url>
      <content type="text"><![CDATA[概述基于Jenkins、Gitlab构建持续集成、持续交付的Docker容器集群。 准备环境 Linux系统（以ubuntu示例） Docker运行环境 docker-compose编排工具 安装Docker安装Docker执行以下命令：1curl -fsSL https://get.docker.io | bash 注：升级内核 安装docker-compose默认的[官方文档][]安装命令如下：1curl -L https://github.com/docker/compose/releases/download/1.6.2/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose 搭建基础服务CI、CD涉及的基础服务有： 代码仓库（以Gitlab示例） 镜像仓库（以搭建私有镜像仓库示例） 代码构建（以Jenkins示例） 容器集群（以swarm示例） 代码仓库（构建Gitlab服务）GitLab是一个利用 Ruby on Rails 开发的开源应用程序，实现一个自托管的Git项目仓库，可通过Web界面进行访问公开的或者私人项目；类似于Github。 选一台主机以docker化的方式来搭建Gitlab服务。（已有私有Gitlab仓库或使用Github，可跳过此步骤） 我们选择Docker Hub上的sameersbn/gitlab镜像来快速构建Gitlab服务。根据sameersbn/docker-gitlab给出的文档，我们需要启动三个容器组件. Docker cli启动方式 启动postgresql容器（可使用外部数据、也可使用mqsql存储，具体参见文档） 1234567docker run --name gitlab-postgresql -d \ --env 'DB_NAME=gitlabhq_production' \ --env 'DB_USER=gitlab' \ --env 'DB_PASS=password' \ --env 'DB_EXTENSION=pg_trgm' \ --volume /srv/docker/gitlab/postgresql:/var/lib/postgresql \ sameersbn/postgresql:9.5-3 启动redis容器 123docker run --name gitlab-redis -d \ --volume /srv/docker/gitlab/redis:/var/lib/redis \ sameersbn/redis:latest 启动gitlab容器 1234567891011docker run --name gitlab -d \ --link gitlab-postgresql:postgresql --link gitlab-redis:redisio \ --publish 10022:22 --publish 10080:80 \ --env 'GITLAB_PORT=10080' \ --env 'GITLAB_SSH_PORT=10022' \ --env 'GITLAB_SECR ETS_DB_KEY_BASE=long-and-random-alpha-numeric-string' \ --env 'GITLAB_SECRETS_SECRET_KEY_BASE=long-and-random-alpha-numeric-string' \ --env 'GITLAB_SECRETS_OTP_KEY_BASE=long-and-random-alpha-numeric-string' \ --volume /srv/docker/gitlab/gitlab:/home/git/data \ sameersbn/gitlab:latest docker-compose启动方式通常处理复杂容器之间的配置依赖关系可以使用docker-compose工具来管理，用法参见docker-compose文档。 使用docker-compose的启动方式，需要docker-compose.yml文件，内容如下：12345678910111213141516171819202122232425262728293031323334gitlab: image: sameersbn/gitlab ports: - "10022:22" - "10080:80" links: - gitlab-redis:redisio - gitlab-postgresql:postgresql environment: - GITLAB_PORT=80 - GITLAB_SSH_PORT=22 - GITLAB_SECRETS_DB_KEY_BASE=long-and-random-alpha-numeric-string - GITLAB_SECRETS_SECRET_KEY_BASE=long-and-random-alpha-numeric-string - GITLAB_SECRETS_OTP_KEY_BASE=long-and-random-alpha-numeric-string volumes: - /srv/docker/gitlab/gitlab:/home/git/data restart: alwaysgitlab-redis: image: sameersbn/redis volumes: - /srv/docker/gitlab/redis:/var/lib/redis restart: alwaysgitlab-postgresql: image: sameersbn/postgresql:9.5-3 environment: - DB_NAME=gitlabhq_production - DB_USER=gitlab - DB_PASS=password - DB_EXTENSION=pg_trgm volumes: - /srv/docker/gitlab/postgresql:/var/lib/postgresql restart: always 执行以下命令，构建gitlab服务1docker-compose up -d 测试执行docker ps命令查看： 访问\:10080，查看Gitlab网页是否正常，并设置root账初始密码。 镜像仓库（构建Docker私有仓库）镜像管理是Docker容器的核心，Docker官方在Github上有一个项目docker-registry, 专门用于自建Docker的私有镜像库。 在CI、CD中，需要一个Docker镜像仓库来存放每次构建的镜像（使用公有镜像仓库如Docker Hub可以自行跳过）。从镜像的安全、可靠、访问速度等因素考虑，搭建一个私有的镜像仓库，对企业级开发和实践是很有必要的。 Docker cli构建方式最简便的创建方式：12docker run -d --restart=always --name registry \ -v /mnt/docker/registry:/var/lib/registry -p 5000:5000 registry:2 docker-compose构建方式 编写docker-compose.yml文件，内容如下： 1234567registry: image: registry:2 ports: - "5000:5000" volumes: - /mnt/docker/registry:/var/lib/registry restart: always 执行docker-compose up -d命令 测试 执行docker ps 可查看registry容器运行状态。 测试推送/拉取 12docker pull 镜像仓库ip:端口/&lt;命名空间&gt;/项目名称 # 拉取镜像docker push 镜像仓库ip:端口/&lt;命名空间&gt;/项目名称 # 推送镜像 注：docker私有镜像仓库 拉取/推送 失败，提示不能使用http连接。解决方法：1、设置https访问，参见企业级的Docker Registry搭建。2、在docker启动参数里添加–insecure-registry \:\，然后重启docker。 镜像构建（搭建Jenkins服务）Jenkins是基于Java开发的一种开源持续集成工具，监控并触发持续重复的工作，具有开源，支持多平台和插件扩展，安装简单，界面化管理等特点。Jenkins使用job来描述每一步工作，节点是用来执行项目的环境。Master节点是Jenkins job的默认执行环境，也是Jenkins应用本身的安装环境使用官方的Jenkins镜像来构建我们的Jenkins服务。 Docker cli构建方式执行以下命令：123docker run docker run -d -p 8080:8080 \--name jenkins --restart=always -v /mnt/jenkins_home:/var/jenkins_home \-v /var/run/docker.sock:/var/run/docker.sock jayqqaa12/jenkins 参数说明：12-v /mnt/jenkins_home:/var/jenkins_home # 映射volumes到本地存储-v /var/run/docker.sock:/var/run/docker.sock # 映射主机的docker到容器里面 这样在容器里面就可以使用主机安装的 docker 了 docker-compose构建方式编写docker-compose.yml文件，内容如下： 12345678jenkins: image: jayqqaa12/jenkins ports: - "8080:8080" volumes: - /var/run/docker.sock:/var/run/docker.sock - /mnt/jenkins_home:/var/jenkins_home restart: always 执行docker-compose up -d命令 测试 执行docker ps 查看jenkins容器运行情况 在浏览器中，打开http://\:8080，进入jenkins，默认用户/密码（admin/admin） 容器集群（构建Docker Swarm集群）容器集群管理工具主要有k8s、mesos、swarm，以及从Docker1.12版本出现的swarmkit。这里以swarm示例容器集群的简单操作。 Swarm是Docker官方发布的一套较为简单的容器集群管理工具，它将一群Docker宿主机变成一个单一的虚拟主机。Swarm使用标准的Docker API接口，因此能可以很方便地和docker的标准API进行集成。 下面，使用Docker-Swarm来创建一个docker容器集群 12345678910#创建一个swarm集群 ，会返回一个token来替换下面命令的&lt;token&gt;docker run --rm swarm create # 创建集群的管理容器swarm 来管理所有节点 docker run -d --name swarm-manage --restart=always -p \2376:2375 swarm manage token://&lt;token&gt; #添加节点到swarm集群中 docker run -d --restart=always --name swarm-agent swarm \join --addr=当前服务器的ip:2375 token://&lt;token&gt; 12#查看集群的节点信息 docker -H 管理节点的ip:2376 info 查看当前集群信息，只有一个节点： 12docker -H 管理节点的ip:2376 run xxx #运行容器docker -H 管理节点的ip:2376 ps xxx #查看集群容器运行情况 以上是使用swarm集群运行、查看容器，更多swarm集群操作详见swarm文档。 注：节点发现无法连接，则需要在Docker daemon添加启动参数：123--insecure-registry &lt;镜像仓库host&gt;:&lt;port&gt; # 设置仓库为http访问 -H unix:///var/run/docker.sock -H 0.0.0.0:2375 #打开节点对外通信接口 --label label_name=swarm-node1 # 设置一下label 方便区分 Demo: 基于Docker构建持续集成（CI）、持续部署（CD）的Flask项目以上，我们搭建了基于Docker的CI、CD的基础服务，下面我们创建一个Flask项目的Demo来测试下： 在gitlab创建python-hello-world项目仓库 clone项目，本地开发 配置jenkins 触发构建 gitlab上新建项目在gitlab创建python-hello-world项目仓库。 本地开发编写Flask应用创建app.py1234567891011from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return "Hello World! &lt;br/&gt; version: v1 author: lain"if __name__ == '__main__': app.run(host="0.0.0.0", port=5000) 创建requirements.txt:1flask 编写Dockerfile创建Dockerfile12345678910111213From python:2.7MAINTAINER lainADD . /appWORKDIR ./appRUN pip install -r requirements.txtEXPOSE 5000CMD ["python", "/app/hello.py"] 创建构建部署脚本创建build_deploy.sh1234567891011121314151617181920212223242526#!/bin/bash#build in jenkins# docker私有仓库的地址REG_URL=xxxx# swarm集群manage节点的地址SWARM_MANAGE_URL=xx:2376#根据时间生成tagTAG=$REG_URL/$JOB_NAME:`date +%y%m%d-%H-%M`#使用项目目录下的Dockerfile文件打包 docker build -t $TAG $WORKSPACE/.docker push $TAGdocker rmi $TAG# 检测是否有运行的版本，有就删了if docker -H $SWARM_MANAGE_URL ps -a| grep -i $JOB_NAME; then docker -H $SWARM_MANAGE_URL rm -f $JOB_NAMEfi#在swarm集群运行docker -H $SWARM_MANAGE_URL run -d -p 80:5000 --name $JOB_NAME $TAG 配置Jenkins在Jenkins上创建一个python-hello-world项目，选择自由风格即可： 设置git: 设置构建触发器，这里设置每分钟拉取一次，也可设置gitlab hook： 设置构建需要执行的脚本，这里设置成build_deploy.sh: 触发CI、CD推送到代码到gitlab的master分支，然后查看jenkins console，就可以查看到构建集成和部署的过程。 访问测试访问浏览器，已经部署到swarm集群（关于集群的负载均衡，请自行查阅。） 更新代码版本，并推送到代码仓库gilab：1234567891011from flask import Flaskapp = Flask(__name__)@app.route('/')def hello_world(): return "Hello World! &lt;br/&gt; version: v2 author: lain"if __name__ == '__main__': app.run(host="0.0.0.0", port=5000) 等待2分钟，可以在jenkins查看到新的构建是否成功，访问浏览器： 容器监控关于采集容器运行信息数据、查看容器运行情况、日志等，通过命令行的方式是很不方便的，因此我们需要搭建一个容器监控平台，开源的工具有如何管理容器 查看容器运行情况 日志等都不太方便 我们需要搭建一个监控平台cAdvisor等。（这里不再叙述）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[基于KS3、Harbor搭建企业级的Docker]]></title>
      <url>%2F2016%2F11%2F07%2Fcreate-ks3-harbor-registry%2F</url>
      <content type="text"><![CDATA[概述Harbor 是 VMware 中国开发的一款 Docker Registry 工具，其主要致力于企业级的 Registry 管理，并提供了 LDAP 等高级权限认证功能。 KS3（Kingsoft Standard Storage Service）是金山云为企业用户提供的无限制、多备份、分布式的低成本存储空间服务，解决存储扩容、数据可靠安全以及分布式访问等相关复杂问题。 本文介绍了如何使用基于KS3存储搭建Harbor企业级的Registry镜像仓库管理。 准备Docker Registry v2需要运行在有Docker环境的主机上，Harbor项目是以容器化的方式来启动的，各组件容器之间的依赖由docker-compose管理。 环境 Linux系统 （本文以ubuntu示例） Docker运行环境 docker-compose编排工具 安装Docker执行以下命令安装 Docker1$ curl -fsSL https://get.docker.io | bash 安装docker-compose默认的官方文档 安装命令如下 :1curl -L https://github.com/docker/compose/releases/download/1.6.2/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose 部署安装部署步骤如下： 下载安装包 配置harbor 配置后端存储到KS3 配置HTTPS 执行安装脚本并启动Harbor 测试 下载安装包Harbor发行版的安装包可以在GItHub下载，有在线（online）安装和离线（offline）安装两种安装方式选择。 以在线安装示例，通过wget命令下载，使用tar命令将其解压，目录结构如下： 12wget https://github.com/vmware/harbor/releases/download/0.4.5/harbor-online-installer-0.4.5.tgztar -zxvf harbor-online-installer-0.4.5.tgz 其中： install.sh 是用于安装Harbor安装脚本config 目录存放了harbor的配置数据，如registry 和 ui目录中存放了相关证书用于组件间加密通讯harbor.cfg 是全局配置文件，主要包含了一些常用设置，比如是否启用https等prepare 是一个python写的预处理脚本，主要用于初始化一些harbor.cfg的相关设置docker-compose.yml 描述了个组件之间依赖关系以及配置挂载，数据持久化等设置。 配置HarborHarbor的常用配置参数主要存放在harbor.cfg，直接编辑harbor.cfg即可： 1234567891011121314151617181920212223242526272829vim harbor.cfg# 其配置信息如下hostname = reg.yourdomain.com # Harbor 服务器域名ui_url_protocol = https # UI 组件访问协议email_server = smtp.mydomain.com # email 服务器地址email_server_port = 25 # email 端口email_username = sample_admin@mydomain.com # email 账号email_password = abc # email 密码email_from = admin &lt;sample_admin@mydomain.com&gt; # email 发件人email_ssl = false # 是否启用 SSLharbor_admin_password = Harbor12345 # Harbor 初始化管理员(admin)密码auth_mode = db_auth # 权限管理模型(db_auth/ldap_auth)ldap_url = ldaps://ldap.mydomain.com # ldap 地址ldap_basedn = uid=%s,ou=people,dc=mydomain,dc=com # ldap 权限模型db_password = root123 # 数据库 管理员密码self_registration = on # 是否打开自动注册use_compressed_js = on # 是否启用压缩jsmax_job_workers = 3 # 最大任务数token_expiration = 30 # token 超时verify_remote_cert = on # 是否验证远程证书customize_crt = on # 是否启用自定义证书# 以下为自定义证书信息crt_country = CNcrt_state = Statecrt_location = CNcrt_organization = organizationcrt_organizationalunit = organizational unitcrt_commonname = example.comcrt_email = example@example.com 配置后端存储到KS3在默认情况下，Harbor的镜像存储在本地磁盘/data/registry目录下。但是在生产环境中，出于高可用、高吞吐、安全可靠等因素，我们可能需要将镜像数据存储在KS3、Ceph等。 金山云提供了基于Docker registry v2封装了KS3驱动的官方镜像ksyun/registry:2。该镜像提供了Docker registry v2和KS3的完美对接，通过该镜像，用户可以很方便将镜像仓库数据存储到KS3上。 使用KS3作为Harbor镜像仓库的后端存储，你需要两步配置：首先，修改harbor/docker-compose文件，修改registry模块，以支持KS3驱动的官方镜像。 1234567891011121314151617181920regsitry: image: ksyun/registry:latest # 修改为支持KS3驱动的镜像 container_name: registry restart: always volumes: # 此处注释掉harbor默认挂在本地目录vlomues的存储方式 # - /data/registry:/storage - ./common/config/registry/:/etc/registry/ environment: - GODEBUG=netdns=cgo command: ["serve", "/etc/registry/config.yml"] depends_on: - log logging: driver: "syslog" options: syslog-address: "tcp://127.0.0.1:1514" tag: "registry"... 其次，配置template/registry/config.yml文件，修改storage模块，为KS3的配置数据。 123456789101112storage: cache: inmemory ks3: accesskey: "your ks3 accesskey" # KS3的访问私钥 secretkey: "your ks3 secretkey" # KS3的访问秘钥 internal: false # 默认值false，是否使用内网加速，true代表使用内网加速，（高速，不计流量） region： ks3-cn-beijing # 区域设置 bucket : "your bucket name" # KS3存储桶的名字 encrypt: false # 默认值false，是否对镜像数据加密 secure: false # 默认值false，是否使用https（ssl） chunksize: 5242880 # 块大小 storage_page: / # 存储路径 更多KS3存储配置，参见Docker Registry KS3存储驱动配置。 HTTPS配置创建CA证书123openssl req \ -newkey rsa:4096 -nodes -sha256 -keyout ca.key \ -x509 -days 365 -out ca.crt 证书签名123openssl req \ -newkey rsa:4096 -nodes -sha256 -keyout yourdomain.com.key \ -out yourdomain.com.csr 初始化CA信息12345mkdir demoCAcd demoCAtouch index.txtecho '01' &gt; serialcd .. 配置Nginx 复制证书 1234567# 复制证书cp registry.mritd.me.crt config/nginx/certcp ca/registry.mritd.me.key config/nginx/cert# 备份配置mv config/nginx/nginx.conf config/nginx/nginx.conf.bak# 使用模板文件mv config/nginx/nginx.https.conf config/nginx/nginx.conf 修改nginx.conf配置 12345678 server &#123; listen 443 ssl; server_name harbordomain.com; ...server &#123; listen 80; server_name harbordomain.com; rewrite ^/(.*) https://$server_name:443/$1 permanent; 执行安装脚本并启动Harbor配置完成后，执行Harbor提供的安装脚本（install.sh），来生成私有配置，在线安装版本会到Docker Hub拉取镜像，并启动Harbor服务。1$ sudo ./install.sh 测试 访问Harbor UI 在浏览器中打开http://reg.yourdomain.com/ ，以admin账号（初始账号/密码：admin/Harbor12345）登录，可以查看Harbor的控制面板。 测试docker client 登录、推拉镜像 在控制面板（Harbor UI）中创建一个项目myproject，docker client登录认证，并测试推送私有镜像： 12$ docker login reg.yourdomain.com$ docker push reg.yourdomain.com/myproject/myrepo:mytag 管理Harbor的生命周期你可以通过docker-compose工具来管理Harbor的生命周期，常用命令如下：1234567891011121314151617181920# 停止Harbor$ sudo docker-compose stop Stopping harbor_proxy_1 ... doneStopping harbor_ui_1 ... doneStopping harbor_registry_1 ... doneStopping harbor_mysql_1 ... doneStopping harbor_log_1 ... doneStopping harbor_jobservice_1 ... done# 启动harbor$ sudo docker-compose startStarting harbor_log_1Starting harbor_mysql_1Starting harbor_registry_1Starting harbor_ui_1Starting harbor_proxy_1Starting harbor_jobservice_1# 更新Harbor配置$ sudo docker-compose down$ vim harbor.cfg$ sudo install.sh 关于docker-compose详见Docker Compose文档 。 MoreKS3更多信息，参见 http://www.ksyun.com/proservice/storage_serviceHarbor更多信息，参见 http://vmware.github.io/harbor/Registry更多信息，参见 https://docker.github.io/registry/Docker更多信息，参见 https://docs.docker.com]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搭建Kubernetes1.4集群]]></title>
      <url>%2F2016%2F10%2F22%2Fcreate-kubernetes-cluster%2F</url>
      <content type="text"><![CDATA[概述Kubernetes 1.4引入了kubeadm的部署机制，极大地简化了Kubernetes集群的构建，可以很方便地集成到自动化运维中（Terraform, Chef, Puppet等）。 kubeadm还处于alpha版本，替换之前kube-up.sh，用于集群的创建和节点的增加。 准备 1、主机集群：一台以上物理机或是VM 2、硬件配置：1G以上内存 3、操作系统：ubuntu 16.04、CentOS7、HypriotOS v1.0.1 4、集群网络：集群所有主机都是连通的（公有或是私有网络） 操作流程安装 kubelet 和 kubeadmKubernetes安装依赖于以下程序包，需要在每个主机（节点）上安装： docker：容器运行环境 kubelet：Kubernetes最核心的组件，它运行在集群所有的节点上，并实际操作POD和容器 kubectl：交互命令行控制集群。通常在Master节点使用，也可在worker节点使用 kubeadm：交互命令行加载集群，用于集群的创建和节点的增加。 Ubuntu/Debian/HypriotOS系统切换至root用户，普通用户执行su -```切换至root用户，执行如下命令：123456789```bash# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -# cat &lt;&lt;EOF &gt; /etc/apt/sources.list.d/kubernetes.listdeb http://apt.kubernetes.io/ kubernetes-xenial mainEOF# apt-get update# # Install docker if you don&apos;t have it already.# apt-get install -y docker.io# apt-get install -y kubelet kubeadm kubectl kubernetes-cni CentOS7系统1234567891011121314# cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=http://yum.kubernetes.io/repos/kubernetes-el7-x86_64enabled=1gpgcheck=1repo_gpgcheck=1gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpgEOF# setenforce 0# yum install -y docker kubelet kubeadm kubectl kubernetes-cni# systemctl enable docker &amp;&amp; systemctl start docker# systemctl enable kubelet &amp;&amp; systemctl start kubelet Note：kubelet 每间隔数秒重启一次，等待接受当出现故障时来自kubeadm的处理响应。 初始化Master节点Master节点运行着“control plane”一组组件，“control plane”的组件主要包括etcd（集群的k-v数据库）和 API Server（提供和kubectl CLI交互），所有的组件都通过kubelet启动运行在pod中。 选择一台安装有kebelet和kubeadm的主机。执行以下命令：kubeadm init```12345678kubeadm init 命令会去安装集群数据库和“control panel”的组件。这个过程会去gcr.io拉取镜像，需要等待几分钟。拉取镜像如下：```bashgcr.io/google_containers/kube-controller-manager-amd64:v1.4.0gcr.io/google_containers/kube-apiserver-amd64:v1.4.0gcr.io/google_containers/etcd-amd64:2.2.5gcr.io/google_containers/pause-amd64:3.0gcr.io/google_containers/kube-scheduler-amd64:v1.4.0 Master节点正确初始化后，会有类似这样的输出：123456789101112131415161718&lt;master/tokens&gt; generated token: "f0c861.753c505740ecde4c"&lt;master/pki&gt; created keys and certificates in "/etc/kubernetes/pki"&lt;util/kubeconfig&gt; created "/etc/kubernetes/kubelet.conf"&lt;util/kubeconfig&gt; created "/etc/kubernetes/admin.conf"&lt;master/apiclient&gt; created API client configuration&lt;master/apiclient&gt; created API client, waiting for the control plane to become ready&lt;master/apiclient&gt; all control plane components are healthy after 61.346626 seconds&lt;master/apiclient&gt; waiting for at least one node to register and become ready&lt;master/apiclient&gt; first node is ready after 4.506807 seconds&lt;master/discovery&gt; created essential addon: kube-discovery&lt;master/addons&gt; created essential addon: kube-proxy&lt;master/addons&gt; created essential addon: kube-dnsKubernetes master initialised successfully!You can connect any number of nodes by running:kubeadm join --token &lt;token&gt; &lt;master-ip&gt; 出于安全考虑，默认情况下Master节点不会调度部署pods，也就是说Master节点不会作为Worker节点，如果你想搭建单机集群，让Master节点成为Worker节点，执行以下命令： 1234# kubectl taint nodes --all dedicated-node "test-01" taintedtaint key="dedicated" and effect="" not found.taint key="dedicated" and effect="" not found 注：这条命令将会从任何节点上移除“dedicated”标记，包括Master节点，这意味着调度器可以在任何节点上调度部署pods。 配置Pod网络Kubernetes 1.2版本默认使用的是flannel网络，用于解决POD跨主机之间的通信。新版本未提供默认的网络插件，在部署应用集群之前，必须要配置POD网络。 未配置POD网络，默认的KUBE-DNS是无法启动的，通过下面的方法查看：1234567#查看系统Pod# kubectl get pods --namespace=kube-system# NAME READY STATUS RESTARTS AGE#kube-dns-2247936740-90wib 0/3 ContainerCreating 0 4m# 查看POD日志# kubectl describe pod kube-dns-2247936740-90wib --namespace=kube-system# ... 这里使用的是weave网络，也可以使用Calico或Cannal。 12# kubectl apply -f https://git.io/weave-kubedaemonset "weave-net" created 网络安装完成，通过下面的命令可以查看kube-dns已经启动12345678910111213141516# kubectl get pods --namespace=kube-system# NAME READY STATUS RESTARTS AGE# etcd-lain-virtual-machine 1/1 Running 0 5m# kube-apiserver-lain-virtual-machine 1/1 Running 0 5m# kube-controller-manager-lain-virtual-machine 1/1 Running 0 5m# kube-discovery-982812725-98ivv 1/1 Running 0 5m# kube-dns-2247936740-90wib 3/3 Running 0 5m# kube-proxy-amd64-pgj8g 1/1 Running 0 5m# kube-scheduler-lain-virtual-machine 1/1 Running 0 5m# weave-net-bkady 2/2 Running 0 5m# 查看启动配置# ps aux | grep kubelet# 输出日志# ... /usr/bin/kubelet --kubeconfig=/etc/kubernetes/kubelet.conf --require-kubeconfig=true --pod-manifest-path=/etc/kubernetes/manifests --allow-privileged=true --network-plugin=cni --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin --cluster-dns=100.64.0.10 --cluster-domain=cluster.local --v=4 添加Worker节点通过join --token ```命令可以添加任意多的节点到Kubernetes集群中。命令操作如下：1234567891011121314151617```bash# kubeadm join --token &lt;token&gt; &lt;master-ip&gt;&lt;util/tokens&gt; validating provided token&lt;node/discovery&gt; created cluster info discovery client, requesting info from &quot;http://138.68.156.129:9898/cluster-info/v1/?token-id=0f8588&quot;&lt;node/discovery&gt; cluster info object received, verifying signature using given token&lt;node/discovery&gt; cluster info signature and contents are valid, will use API endpoints [https://138.68.156.129:443]&lt;node/csr&gt; created API client to obtain unique certificate for this node, generating keys and certificate signing request&lt;node/csr&gt; received signed certificate from the API server, generating kubelet configuration&lt;util/kubeconfig&gt; created &quot;/etc/kubernetes/kubelet.conf&quot;Node join complete:* Certificate signing request sent to master and response received.* Kubelet informed of new secure connection details.Run &apos;kubectl get nodes&apos; on the master to see this machine join. 添加worker节点后，可以在Master节点通过get nodes```查看集群里的机器。1234567### 从Master节点以外的机器控制集群如果你想在笔记本上通过kubectl来控制你的集群，只需要将Master节点上KubeConfig复制到你的笔记本即可，操作如下：```bash# scp root@&lt;master ip&gt;:/etc/kubernetes/admin.conf .# kubectl --kubeconfig ./admin.conf get nodes 安装Demo应用12# kubectl create namespace sock-shop# kubectl apply -n sock-shop -f "https://github.com/microservices-demo/microservices-demo/blob/master/deploy/kubernetes/complete-demo.yaml?raw=true" Tear down uninstall 一个app，在Master节点执行delete namespace sock-shop```123456- undo kubeadm的操作，重置本地状态:```bash systemctl stop kubelet;docker rm -f -v $(docker ps -q);find /var/lib/kubelet | xargs -n 1 findmnt -n -t tmpfs -o TARGET -T | uniq | xargs -r umount -v;rm -r -f /etc/kubernetes /var/lib/kubelet /var/lib/etcd; More 学习Kubeadm的高级用法 http://kubernetes.io/docs/admin/kubeadm/ 学习更多Kubernetes理念和kubctl的用法： http://kubernetes.io/docs/user-guide/walkthrough/ 学习其他组件（包括日志、监控、网络策略、可视化控制等），参见: http://kubernetes.io/docs/admin/addons.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[VMware收缩Linux虚拟机体积]]></title>
      <url>%2F2015%2F12%2F26%2Fvmware-toolbox-shrink%2F</url>
      <content type="text"><![CDATA[虚拟机使用一段时间后体积会越来越大，特别是进行了大程序编译等很占空间的行为后，虚拟磁盘文件经常会占用数十个G的空间。而且就算之后删除了无用文件，虚拟磁盘文件的体积也不会自动缩小。此时，就需要借助VMware Tools进行磁盘空间收缩。 在正确安装了VMware Tools 的前提下，root 账户下执行一下命令：1vmware-toolbox-cmd disk shrink / 命令中最后一个参数是虚拟磁盘的挂载点，一般就是 / 。最后若是出现 “disk shrinking complete” 即代表压缩完成，此时在WIndows资源管理器中即可看到虚拟磁盘文件的体积显著缩小，基本上就与虚拟机实际已使用空间一样大（可使用 df 确认）。 若输入上述命令后，提示 “Shrink disk is disabled for this virtual machine.”, 需要检查是否存在快照（snapshot）、是否被预分配（preallocated）、是否存在不能收缩的物理硬盘等情况。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Gevent简明教程]]></title>
      <url>%2F2015%2F11%2F28%2Fpython-gevent%2F</url>
      <content type="text"><![CDATA[前述进程 线程 协程 异步并发编程（不是并行）目前有四种方式：多进程、多线程、协程和异步。 多进程编程在python中有类似C的os.fork,更高层封装的有multiprocessing标准库 多线程编程python中有Thread和threading 异步编程在linux下主+要有三种实现select，poll，epoll 协程在python中通常会说到yield，关于协程的库主要有greenlet,stackless,gevent,eventlet等实现。 进程 不共享任何状态 调度由操作系统完成 有独立的内存空间（上下文切换的时候需要保存栈、cpu寄存器、虚拟内存、以及打开的相关句柄等信息，开销大） 通讯主要通过信号传递的方式来实现（实现方式有多种，信号量、管道、事件等，通讯都需要过内核，效率低） 线程 共享变量（解决了通讯麻烦的问题，但是对于变量的访问需要加锁） 调度由操作系统完成（由于共享内存，上下文切换变得高效） 一个进程可以有多个线程，每个线程会共享父进程的资源（创建线程开销占用比进程小很多，可创建的数量也会很多） 通讯除了可使用进程间通讯的方式，还可以通过共享内存的方式进行通信（通过共享内存通信比通过内核要快很多） 协程 调度完全由用户控制 一个线程（进程）可以有多个协程 每个线程（进程）循环按照指定的任务清单顺序完成不同的任务（当任务被堵塞时，执行下一个任务；当恢复时，再回来执行这个任务；任务间切换只需要保存任务的上下文，没有内核的开销，可以不加锁的访问全局变量） 协程需要保证是非堵塞的且没有相互依赖 协程基本上不能同步通讯，多采用异步的消息通讯，效率比较高 总结 进程拥有自己独立的堆和栈，既不共享堆，亦不共享栈，进程由操作系统调度 线程拥有自己独立的栈和共享的堆，共享堆，不共享栈，线程亦由操作系统调度(标准线程是的) 协程和线程一样共享堆，不共享栈，协程由程序员在协程的代码里显示调度 聊聊协程协程，又称微线程，纤程。Python的线程并不是标准线程，是系统级进程，线程间上下文切换有开销，而且Python在执行多线程时默认加了一个全局解释器锁（GIL），因此Python的多线程其实是串行的，所以并不能利用多核的优势，也就是说一个进程内的多个线程只能使用一个CPU。 def coroutine(func): def ret(): f = func() f.next() return f return ret @coroutine def consumer(): print &quot;Wait to getting a task&quot; while True: n = (yield) print &quot;Got %s&quot;,n import time def producer(): c = consumer() task_id = 0 while True: time.sleep(1) print &quot;Send a task to consumer&quot; % task_id c.send(&quot;task %s&quot; % task_id) if __name__ == &quot;__main__&quot;: producer() 结果： Wait to getting a task Send a task 0 to consumer Got task 0 Send a task 1 to consumer Got task 1 Send a task 2 to consumer Got task 2 ... 传统的生产者-消费者模型是一个线程写消息，一个线程取消息，通过锁机制控制队列和等待，但容易死锁。如果改用协程，生产者生产消息后，直接通过yield跳转到消费者开始执行，待消费者执行完毕后，切换回生产者继续生产，效率极高。 Gevent介绍gevent是基于协程的Python网络库。特点： 基于libev的快速事件循环(Linux上epoll，FreeBSD上kqueue）。 基于greenlet的轻量级执行单元。 API的概念和Python标准库一致(如事件，队列)。 可以配合socket，ssl模块使用。 能够使用标准库和第三方模块创建标准的阻塞套接字(gevent.monkey)。 默认通过线程池进行DNS查询,也可通过c-are(通过GEVENT_RESOLVER=ares环境变量开启）。 TCP/UDP/HTTP服务器 子进程支持（通过gevent.subprocess） 线程池 安装和依赖依赖于greenlet library支持python 2.6+ 、3.3+ 核心部分 Greenlets 同步和异步执行 确定性 创建Greenlets Greenlet状态 程序停止 超时 猴子补丁 ####Greenletsgevent中的主要模式, 它是以C扩展模块形式接入Python的轻量级协程。 全部运行在主程序操作系统进程的内部，但它们被程序员协作式地调度。 在任何时刻，只有一个协程在运行。 区别于multiprocessing、threading等提供真正并行构造的库， 这些库轮转使用操作系统调度的进程和线程，是真正的并行。 同步和异步执行并发的核心思想在于，大的任务可以分解成一系列的子任务，后者可以被调度成 同时执行或异步执行，而不是一次一个地或者同步地执行。两个子任务之间的 切换也就是上下文切换。 在gevent里面，上下文切换是通过yielding来完成的.12345678910111213141516import geventdef foo(): print('Running in foo') gevent.sleep(0) print('Explicit context switch to foo again')def bar(): print('Explicit context to bar') gevent.sleep(0) print('Implicit context switch back to bar')gevent.joinall([ gevent.spawn(foo), gevent.spawn(bar),]) 执行结果：1234Running in fooExplicit context to barExplicit context switch to foo againImplicit context switch back to bar 代码执行过程： 网络延迟或IO阻塞隐式交出greenlet上下文的执行权。1234567891011121314151617181920212223242526import timeimport geventfrom gevent import selectstart = time.time()tic = lambda: 'at %1.1f seconds' % (time.time() - start)def gr1(): print('Started Polling: %s' % tic()) select.select([], [], [], 1) print('Ended Polling: %s' % tic())def gr2(): print('Started Polling: %s' % tic()) select.select([], [], [], 2) print('Ended Polling: %s' % tic())def gr3(): print("Hey lets do some stuff while the greenlets poll, %s" % tic()) gevent.sleep(1)gevent.joinall([ gevent.spawn(gr1), gevent.spawn(gr2), gevent.spawn(gr3),]) 执行结果：12345Started Polling: at 0.0 secondsStarted Polling: at 0.0 secondsHey lets do some stuff while the greenlets poll, at 0.0 secondsEnded Polling: at 1.0 secondsEnded Polling: at 2.0 seconds 同步vs异步1234567891011121314151617181920import geventimport randomdef task(pid): gevent.sleep(random.randint(0,2)*0.001) print('Task %s done' % pid)def synchronous(): for i in xrange(5): task(i)def asynchronous(): threads = [gevent.spawn(task, i) for i in xrange(5)] gevent.joinall(threads) print('Synchronous:') synchronous() print('Asynchronous:') asynchronous() 执行结果：123456789101112Synchronous:Task 0 doneTask 1 doneTask 2 doneTask 3 doneTask 4 doneAsynchronous:Task 2 doneTask 0 doneTask 1 doneTask 3 doneTask 4 done 确定性greenlet具有确定性。在相同配置相同输入的情况下，它们总是会产生相同的输出。 123456789101112131415161718192021222324252627import timedef echo(i): time.sleep(0.001) return i# Non Deterministic Process Poolfrom multiprocessing.pool import Poolp = Pool(10)run1 = [a for a in p.imap_unordered(echo, xrange(10))]run2 = [a for a in p.imap_unordered(echo, xrange(10))]run3 = [a for a in p.imap_unordered(echo, xrange(10))]run4 = [a for a in p.imap_unordered(echo, xrange(10))]print(run1 == run2 == run3 == run4)# Deterministic Gevent Poolfrom gevent.pool import Poolp = Pool(10)run1 = [a for a in p.imap_unordered(echo, xrange(10))]run2 = [a for a in p.imap_unordered(echo, xrange(10))]run3 = [a for a in p.imap_unordered(echo, xrange(10))]run4 = [a for a in p.imap_unordered(echo, xrange(10))]print(run1 == run2 == run3 == run4) 执行结果：12FalseTrue 即使gevent通常带有确定性，当开始与如socket或文件等外部服务交互时， 不确定性也可能溜进你的程序中。因此尽管gevent线程是一种“确定的并发”形式， 使用它仍然可能会遇到像使用POSIX线程或进程时遇到的那些问题。 涉及并发长期存在的问题就是竞争条件(race condition)(当两个并发线程/进程都依赖于某个共享资源同时都尝试去修改它的时候， 就会出现竞争条件),这会导致资源修改的结果状态依赖于时间和执行顺序。 这个问题，会导致整个程序行为变得不确定。 解决办法: 始终避免所有全局的状态. 创建Greenletsgevent对Greenlet初始化提供了一些封装.123456789101112import geventfrom gevent import Greenletdef foo(message, n): gevent.sleep(n) print(message) thread1 = Greenlet.spawn(foo, "Hello", 1) thread2 = gevent.spawn(foo, "I live!", 2) thread3 = gevent.spawn(lambda x: (x+1), 2) threads = [thread1, thread2, thread3] gevent.joinall(threads) 执行结果：12HelloI live! 除使用基本的Greenlet类之外，你也可以子类化Greenlet类，重载它的_run方法。1234567891011121314151617import geventfrom gevent import Greenletclass MyGreenlet(Greenlet): def __init__(self, message, n): Greenlet.__init__(self) self.message = message self.n = n def _run(self): print(self.message) gevent.sleep(self.n)g = MyGreenlet("Hi there!", 3)g.start()g.join() 执行结果：1Hi there! Greenlet状态greenlet的状态通常是一个依赖于时间的参数： started – Boolean, 指示此Greenlet是否已经启动 ready() – Boolean, 指示此Greenlet是否已经停止 successful() – Boolean, 指示此Greenlet是否已经停止而且没抛异常 value – 任意值, 此Greenlet代码返回的值 exception – 异常, 此Greenlet内抛出的未捕获异常 程序停止程序当主程序(main program)收到一个SIGQUIT信号时，不能成功做yield操作的 Greenlet可能会令意外地挂起程序的执行。这导致了所谓的僵尸进程， 它需要在Python解释器之外被kill掉。 通用的处理模式就是在主程序中监听SIGQUIT信号，调用gevent.shutdown退出程序。12345678910import geventimport signaldef run_forever(): gevent.sleep(1000) if __name__ == '__main__': gevent.signal(signal.SIGQUIT, gevent.shutdown) thread = gevent.spawn(run_forever) thread.join() 超时通过超时可以对代码块儿或一个Greenlet的运行时间进行约束。123456789101112131415import geventfrom gevent import Timeoutseconds = 10timeout = Timeout(seconds)timeout.start()def wait(): gevent.sleep(10) try: gevent.spawn(wait).join() except Timeout: print('Could not complete') 超时类12345678910import geventfrom gevent import Timeouttime_to_wait = 5 # seconds class TooLong(Exception): pass with Timeout(time_to_wait, TooLong): gevent.sleep(10) 另外，对各种Greenlet和数据结构相关的调用，gevent也提供了超时参数。123456789101112131415161718192021222324252627282930import geventfrom gevent import Timeoutdef wait(): gevent.sleep(2)timer = Timeout(1).start()thread1 = gevent.spawn(wait)try: thread1.join(timeout=timer)except Timeout: print('Thread 1 timed out')# --timer = Timeout.start_new(1)thread2 = gevent.spawn(wait)try: thread2.get(timeout=timer)except Timeout: print('Thread 2 timed out')# --try: gevent.with_timeout(1, wait)except Timeout: print('Thread 3 timed out') 执行结果：123Thread 1 timed outThread 2 timed outThread 3 timed out 猴子补丁(Monkey patching)gevent的死角.12345678910111213import socketprint(socket.socket)print("After monkey patch")from gevent import monkeymonkey.patch_socket()print(socket.socket)import selectprint(select.select)monkey.patch_select()print("After monkey patch")print(select.select) 执行结果：1234567class 'socket.socket'After monkey patchclass 'gevent.socket.socket'built-in function selectAfter monkey patchfunction select at 0x1924de8 Python的运行环境允许我们在运行时修改大部分的对象，包括模块，类甚至函数。 这是个一般说来令人惊奇的坏主意，因为它创造了“隐式的副作用”，如果出现问题 它很多时候是极难调试的。虽然如此，在极端情况下当一个库需要修改Python本身 的基础行为的时候，猴子补丁就派上用场了。在这种情况下，gevent能够修改标准库里面大部分的阻塞式系统调用，包括socket、ssl、threading和 select等模块，而变为协作式运行。 例如，Redis的python绑定一般使用常规的tcp socket来与redis-server实例通信。 通过简单地调用gevent.monkey.patch_all()，可以使得redis的绑定协作式的调度 请求，与gevent栈的其它部分一起工作。 这让我们可以将一般不能与gevent共同工作的库结合起来，而不用写哪怕一行代码。 虽然猴子补丁仍然是邪恶的(evil)，但在这种情况下它是“有用的邪恶(useful evil)”。 数据结构 事件 队列 组和池 锁和信号量 线程局部变量 子进程 Actors事件事件(event)是一个在Greenlet之间异步通信的形式。1234567891011121314151617181920212223242526import geventfrom gevent.event import Event evt = Event() def setter(): print('A: Hey wait for me, I have to do something') gevent.sleep(3) print("Ok, I'm done") evt.set()def waiter(): print("I'll wait for you") evt.wait() # blocking print("It's about time")def main(): gevent.joinall([ gevent.spawn(setter), gevent.spawn(waiter), gevent.spawn(waiter), gevent.spawn(waiter) ])if __name__ == '__main__': main() 执行结果：12345678A: Hey wait for me, I have to do somethingI&apos;ll wait for youI&apos;ll wait for youI&apos;ll wait for youOk, I&apos;m doneIt&apos;s about timeIt&apos;s about timeIt&apos;s about time 事件对象的一个扩展是AsyncResult，它允许你在唤醒调用上附加一个值。 它有时也被称作是future或defered，因为它持有一个指向将来任意时间可设置为任何值的引用。123456789101112131415import geventfrom gevent.event import AsyncResulta = AsyncResult()def setter(): gevent.sleep(3) a.set('Hello!')def waiter(): print(a.get())gevent.joinall([ gevent.spawn(setter), gevent.spawn(waiter),]) 队列队列是一个排序的数据集合，它有常见的put / get操作， 但是它是以在Greenlet之间可以安全操作的方式来实现的。1234567891011121314151617181920212223import geventfrom gevent.queue import Queuetasks = Queue()def worker(n): while not tasks.empty(): task = tasks.get() print('Worker %s got task %s' % (n, task)) gevent.sleep(0) print('Quitting time!')def boss(): for i in xrange(1,10): tasks.put_nowait(i)gevent.spawn(boss).join()gevent.joinall([ gevent.spawn(worker, 'steve'), gevent.spawn(worker, 'john'), gevent.spawn(worker, 'nancy'),]) 执行结果：123456789101112Worker steve got task 1Worker john got task 2Worker nancy got task 3Worker steve got task 4Worker john got task 5Worker nancy got task 6Worker steve got task 7Worker john got task 8Worker nancy got task 9Quitting time!Quitting time!Quitting time! put和get操作都是阻塞的，put_nowait和get_nowait不会阻塞， 然而在操作不能完成时抛出gevent.queue.Empty或gevent.queue.Full异常。 组和池组(group)是一个运行中greenlet集合，集合中的greenlet像一个组一样会被共同管理和调度。 它也兼饰了像Python的multiprocessing库那样的平行调度器的角色，主要用在在管理异步任务的时候进行分组。123456789101112131415161718import geventfrom gevent.pool import Groupdef talk(msg): for i in xrange(2): print(msg)g1 = gevent.spawn(talk, 'bar')g2 = gevent.spawn(talk, 'foo')g3 = gevent.spawn(talk, 'fizz')group = Group()group.add(g1)group.add(g2)group.join()group.add(g3)group.join() 执行结果：123456barbarfoofoofizzfizz 池(pool)是一个为处理数量变化并且需要限制并发的greenlet而设计的结构。123456789import geventfrom gevent.pool import Poolpool = Pool(2)def hello_from(n): print('Size of pool %s' % len(pool)) pool.map(hello_from, xrange(3)) 执行结果：123Size of pool 2Size of pool 2Size of pool 1 构造一个socket池的类，在各个socket上轮询。1234567891011121314151617181920from gevent.pool import Poolclass SocketPool(object): def __init__(self): self.pool = Pool(10) self.pool.start() def listen(self, socket): while True: socket.recv() def add_handler(self, socket): if self.pool.full(): raise Exception("At maximum pool size") else: self.pool.spawn(self.listen, socket) def shutdown(self): self.pool.kill() 锁和信号量信号量是一个允许greenlet相互合作，限制并发访问或运行的低层次的同步原语。 信号量有两个方法，acquire和release。在信号量是否已经被 acquire或release，和拥有资源的数量之间不同，被称为此信号量的范围 (the bound of the semaphore)。如果一个信号量的范围已经降低到0，它会 阻塞acquire操作直到另一个已经获得信号量的greenlet作出释放。123456789101112131415161718192021from gevent import sleepfrom gevent.pool import Poolfrom gevent.coros import BoundedSemaphoresem = BoundedSemaphore(2)def worker1(n): sem.acquire() print('Worker %i acquired semaphore' % n) sleep(0) sem.release() print('Worker %i released semaphore' % n)def worker2(n): with sem: print('Worker %i acquired semaphore' % n) sleep(0) print('Worker %i released semaphore' % n)pool = Pool()pool.map(worker1, xrange(0,2)) 执行结果：1234Worker 0 acquired semaphoreWorker 1 acquired semaphoreWorker 0 released semaphoreWorker 1 released semaphore 锁(lock)是范围为1的信号量。它向单个greenlet提供了互斥访问。 信号量和锁常被用来保证资源只在程序上下文被单次使用。 线程局部变量Gevent允许程序员指定局部于greenlet上下文的数据。 在内部，它被实现为以greenlet的getcurrent()为键， 在一个私有命名空间寻址的全局查找。12345678910111213141516171819202122import geventfrom gevent.local import localstash = local()def f1(): stash.x = 1 print(stash.x)def f2(): stash.y = 2 print(stash.y) try: stash.x except AttributeError: print("x is not local to f2")g1 = gevent.spawn(f1)g2 = gevent.spawn(f2)gevent.joinall([g1, g2]) 执行结果：12312x is not local to f2 很多集成了gevent的web框架将HTTP会话对象以线程局部变量的方式存储在gevent内。 例如使用Werkzeug实用库和它的proxy对象，我们可以创建Flask风格的请求对象。1234567891011121314151617181920212223242526272829303132from gevent.local import localfrom werkzeug.local import LocalProxyfrom werkzeug.wrappers import Requestfrom contextlib import contextmanagerfrom gevent.wsgi import WSGIServer_requests = local()request = LocalProxy(lambda: _requests.request)@contextmanagerdef sessionmanager(environ): _requests.request = Request(environ) yield _requests.request = Nonedef logic(): return "Hello " + request.remote_addrdef application(environ, start_response): status = '200 OK' with sessionmanager(environ): body = logic() headers = [ ('Content-Type', 'text/html') ] start_response(status, headers) return [body] WSGIServer(('', 8000), application).serve_forever() 子进程从gevent 1.0起，支持gevent.subprocess，支持协作式的等待子进程。123456789101112131415import geventfrom gevent.subprocess import Popen, PIPEdef cron(): while True: print("cron") gevent.sleep(0.2)g = gevent.spawn(cron)sub = Popen(['sleep 1; uname'], stdout=PIPE, shell=True)out, err = sub.communicate()g.kill()print(out.rstrip())``` 执行结果： cron cron cron cron cron Linux 12345678910111213141516171819202122232425262728293031323334353637383940414243很多人也想将gevent和multiprocessing一起使用。最明显的挑战之一 就是multiprocessing提供的进程间通信默认不是协作式的。由于基于 multiprocessing.Connection的对象(例如Pipe)暴露了它们下面的 文件描述符(file descriptor)，gevent.socket.wait_read和wait_write 可以用来在直接读写之前协作式的等待ready-to-read/ready-to-write事件。```pythonimport geventfrom multiprocessing import Process, Pipefrom gevent.socket import wait_read, wait_write# To Processa, b = Pipe()# From Processc, d = Pipe()def relay(): for i in xrange(5): msg = b.recv() c.send(msg + &quot; in &quot; + str(i))def put_msg(): for i in xrange(5): wait_write(a.fileno()) a.send(&apos;hi&apos;)def get_msg(): for i in xrange(5): wait_read(d.fileno()) print(d.recv())if __name__ == &apos;__main__&apos;: proc = Process(target=relay) proc.start() g1 = gevent.spawn(get_msg) g2 = gevent.spawn(put_msg) gevent.joinall([g1, g2], timeout=1)``` 执行结果：``` hi in 0hi in 1hi in 2hi in 3hi in 4 然而要注意，组合multiprocessing和gevent必定带来 依赖于操作系统(os-dependent)的缺陷，其中有： 在兼容POSIX的系统创建子进程(forking)之后， 在子进程的gevent的状态是不适定的(ill-posed)。一个副作用就是， multiprocessing.Process创建之前的greenlet创建动作，会在父进程和子进程两方都运行。 上例的put_msg()中的a.send()可能依然非协作式地阻塞调用的线程：一个 ready-to-write事件只保证写了一个byte。在尝试写完成之前底下的buffer可能是满的。 上面表示的基于wait_write()/wait_read()的方法在Windows上不工作 (IOError: 3 is not a socket (files are not supported))，因为Windows不能监视 pipe事件。 Python包gipc以大体上透明的方式在 兼容POSIX系统和Windows上克服了这些挑战。它提供了gevent感知的基于 multiprocessing.Process的子进程和gevent基于pipe的协作式进程间通信。 Actorsactor模型是一个由于Erlang变得普及的更高层的并发模型。 简单的说它的主要思想就是许多个独立的Actor，每个Actor有一个可以从 其它Actor接收消息的收件箱。Actor内部的主循环遍历它收到的消息，并根据它期望的行为来采取行动。 Gevent没有原生的Actor类型，但在一个子类化的Greenlet内使用队列， 我们可以定义一个非常简单的。123456789101112131415161718192021import geventfrom gevent.queue import Queueclass Actor(gevent.Greenlet): def __init__(self): self.inbox = Queue() Greenlet.__init__(self) def receive(self, message): """ Define in your subclass. """ raise NotImplemented() def _run(self): self.running = True while self.running: message = self.inbox.get() self.receive(message) 下面是一个使用的例子：123456789101112131415161718192021222324import geventfrom gevent.queue import Queuefrom gevent import Greenletclass Pinger(Actor): def receive(self, message): print(message) pong.inbox.put('ping') gevent.sleep(0)class Ponger(Actor): def receive(self, message): print(message) ping.inbox.put('pong') gevent.sleep(0)ping = Pinger()pong = Ponger()ping.start()pong.start()ping.inbox.put('start')gevent.joinall([ping, pong]) 实际应用 Gevent ZeroMQ 简单server WSGI Servers 流式server Long Polling Websockets 简单server12345678910111213# On Unix: Access with ``$ nc 127.0.0.1 5000``# On Window: Access with ``$ telnet 127.0.0.1 5000``from gevent.server import StreamServerdef handle(socket, address): socket.send("Hello from a telnet!\n") for i in range(5): socket.send(str(i) + '\n') socket.close()server = StreamServer(('127.0.0.1', 5000), handle)server.serve_forever() WSGI Servers And WebsocketsGevent为HTTP内容服务提供了两种WSGI server。从今以后就称为 wsgi和pywsgi： gevent.wsgi.WSGIServer gevent.pywsgi.WSGIServer glb中使用123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import clickfrom flask import Flaskfrom gevent.pywsgi import WSGIServerfrom geventwebsocket.handler import WebSocketHandlerimport v1from .settings import Configfrom .sockethandler import handle_websocketdef create_app(config=None): app = Flask(__name__, static_folder='static') if config: app.config.update(config) else: app.config.from_object(Config) app.register_blueprint( v1.bp, url_prefix='/v1') return appdef wsgi_app(environ, start_response): path = environ['PATH_INFO'] if path == '/websocket': handle_websocket(environ['wsgi.websocket']) else: return create_app()(environ, start_response)@click.command()@click.option('-h', '--host_port', type=(unicode, int), default=('0.0.0.0', 5000), help='Host and port of server.')@click.option('-r', '--redis', type=(unicode, int, int), default=('127.0.0.1', 6379, 0), help='Redis url of server.')@click.option('-p', '--port_range', type=(int, int), default=(50000, 61000), help='Port range to be assigned.')def manage(host_port, redis=None, port_range=None): Config.REDIS_URL = 'redis://%s:%s/%s' % redis Config.PORT_RANGE = port_range http_server = WSGIServer(host_port, wsgi_app, handler_class=WebSocketHandler) print '----GLB Server run at %s:%s-----' % host_port print '----Redis Server run at %s:%s:%s-----' % redis http_server.serve_forever() 缺陷和其他异步I/O框架一样,gevent也有一些缺陷: 阻塞(真正的阻塞,在内核级别)在程序中的某个地方停止了所有的东西.这很像C代码中monkey patch没有生效 保持CPU处于繁忙状态.greenlet不是抢占式的,这可能导致其他greenlet不会被调度. 在greenlet之间存在死锁的可能. 一个gevent回避的缺陷是,你几乎不会碰到一个和异步无关的Python库–它将阻塞你的应用程序,因为纯Python库使用的是monkey patch的stdlib.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Elasticsearch配置及优化]]></title>
      <url>%2F2014%2F12%2F15%2Felasticsearch-config-optimize%2F</url>
      <content type="text"><![CDATA[环境变量在es启动脚本里（elasticsearch.in.sh或elasticsearch.in.bat），内置了传递给JVM的启动参数JAVA_OPTS。其中最重要的参数是 -Xmx 和 -Xms，分别用来控制分配给es进程的最大内存、最小内存。（一般来说内存越多越好） 通常来说，JAVA_OPTS使用默认值不要修改，而是通过ES_JAVA_OPTS环境变量来设置和修改JVM配置参数。 ES_HEAP_SIZE环境变量用来设置分配给es的java进程的堆内存，设置了ES_HEAP_SIZE变量它也会同事设置最小、最大内存。也可以单独分别设置ES_MIN_MEM、ES_MAX_MEM参数来决定分配的最小、最大内存。 推荐最小、最大内存设置成相同值，并且打开mlockall开关。 系统配置文件描述符（File Description）确保把机器的（或当前运行es用户的）打开文件描述符数量设置成32k或者推荐的64k。设置完成后，可以通过增加 -Des.max-open-files=true启动参数，打印出进程能打开的文件数，测试是否生效。或者通过Nodes Info Api来获取max_file_descriptors参数：1curl localhost:9200/_nodes/process?pretty 虚拟内存（Virtual memory）es默认使用 hybrid mmapfs / niofs 目录来存储索引。默认操作系统对mmap技术的限制太低，可能引发内存不足的异常。在Linux中，可以通过root账户运行下面的明亮来放开限制：1sysctl -w vm.max_mapcount=262144 永久生效需要更新系统文件 /etc/sysctl.conf 的 vm.max_map_count 字段。如果是通过deb或是rpm包安装的话，会自动更新这个设置，可以通过下面的命令查看：1sysctl vm.max_map_count 内存设置大部分的操作系统会对文件系统使用尽可能多的内存，并且容易换出一些未使用的应用内存，导致es进程内存被换出。这种操作对性能和节点的稳定性影响很大，应尽量避免。避免方法如下： 关闭swap这是最简单的方法。因为通常来说，es是机器上面唯一的服务，它的内存使用由ES_HEAP_SIZE环境变量来控制，没有必要打开swap。 在linux系统，可以通过运行 sudo swapoff -a 命令来临时关闭swap。如果想永久关闭，需要修改 /etc/fstab 文件，注释掉swap的相关配置。 在windows系统，可以通过系统属性-&gt;高级-&gt;性能-&gt;高级-&gt;虚拟内存 来关闭分页文件 配置swappiness通过设置使得sysctl 的 vm.swappiness 为0。这会减少内核交换操作，在通过情况下不会触发swapping，除非在紧急情况下。 注意：从内核版本3.5-rc1开始，swappiness为0将导致OOM killer杀死进程而不是允许交换。你必须把swappiness设为1，使得在紧急情况下可以swapping。 mlockcall通过使用Linux的 mlockcall 或者 Windows的VirtualLock来把进程地址空间锁在内存里面，防止es的内存被换出。通过配置文件 config/elasticsearch.yml 中增加如下配置：1bootstrp.mlockall: true 启动es后，你可以观察这个设置是否生效，通过以下放大检查mlockall：1curl http://localhost:9200/_nodes/process?pretty 如果mlockall为false，则mlockcall失败。最可能的原因是，在Linux/Unix系统中，运行es的用户没有权限来锁住内存，需要在启动es前铜鼓root账户运行 ulimit -l unlimited。 另一个可能的原因是，临时目录（通常是/tmp）被以 noexec 情况下挂载，需要通过以下方法重新指定一个临时目录：1./bin/elasticsearch -Djna.tmpdir=/path/to/new/dir 警告： mloackall可能导致JVM或者shell退出，如果尝试分配多余可用内存的空间。 elasticsearch 配置es配置文件在ES_HOME/config目录下面。elasticsearch.yml用于配置es的不同模块，logging.yml用于配置es的日志。配置文件格式是YAML。下面是一个用于绑定和发布的网络模块的配置：12network: host: 10.0.0.4 Paths（路径）在使用过程中，一般会改变数据和日志的存放路径123path: logs: /var/log/elasticsearch data: /var/data/elasticsearch Cluster name（集群名）记住要设置集群名，集群名用于发现和自动连接其他节点：12cluster: name: 集群名 确保不要在不同的环境使用相同的集群名，否则将导致节点连接进错误的集群。例如：你可以分别使用logging-dev，logging-stage，logging-prod作为开发、测试、正式发布集群。 Node name（节点名）你可能需要修改默认的节点名，比如修改成主机名。如果没有设置的话，es将从3000个名字中随机选一个作为节点名。12node: name: &lt;节点名&gt; 极其的主机名存于环境变量HOSTNAME中。如果集群中，你只运行单节点的es服务，可以把节点设置成主机名：12node: name: $&#123;HOSTNAME&#125; 在内部，所有设置都变成基于命名空间的设置。例如，上述节点配置编程node.name。这意味着可以很容易支持一些其他配置格式，如JSON。如果要使用json配置，只要把elasticsearch.yml文件改成elasticsearch.json并添加：配置格式：12345&#123; "network": &#123; "host": "10.0.0.4" &#125;&#125; 也可以很容易地通过外部采纳数ES_JAVA_OPT或者elasticsearch启动参数来设置，例如：1$ elasticsearch -Des.network.host=10.0.0.4 另外一种选项是设置 es.default. 前缀来代替 es. 前缀，这样使得默认配置只有当前配置文件中没有明确设置时才生效。另外一种选项是在配置文件中使用 $(…) 符号，这些符号与环境设置中变量相对应的，例如：12345&#123; "network": &#123; "host": "$&#123;ES_NET_HOST&#125;" &#125;&#125; 另外，如果你不想把一些参数保存在配置文件中，你可以使用${prompt.text} 或者 ${prompt.secret}，然后前台重启es。${prompt.text}会被显示出来，例如：12node: name: $&#123;prompt.text&#125; 在启动es时，你需要输入对应的值：1Enter value for [node.name]: 说明： 如果配置中使用了${prompt.text} 或者 ${prompt.secret}, es就不能以服务或者后台启动的方式运行。 索引配置集群中的索引可以有自己的配置。例如，一下语句创建一个基于内存的索引，而不是默认基于文件的（可以是YAML或者JSON格式）：12345$ curl -XPUT http://localhost:9200/kimchy/ -d \' index : refresh_interval: 5s' 索引级配置也可以在节点上设置，例如，在elasticsearch.yml文件中，可以配置如下：12index: refresh_interval: 5s 这表明这个节点上创建的索引都是存储在内存中的，除非重新设置。换句话说，索引级配置会覆盖节点级配置。当然上面也可以使用这种格式的配置：1$ elasticsearch -Des.indexrefresh_interval=5s 所有索引级配置都可以在每个索引模块中找到。 日志配置es使用一种内部日志，有点儿像log4j。它通过使用YAML简化了log4j配置，配置文件中config/logging.yml.支持JSON和属性格式。可以加载多个配置文件，只要文件以logging. 前缀开头以.yml，.yaml， .json或者.properties结尾，它们将被合并。logger段中包含了java包和相应的日志级别，可以忽略org.elasticsearch前缀。appender段中包含日志的路径。更多资料在log4j.documentation. 其它Appenders和其他的log4j-extras提供的日志类也可用。 废弃日志除了正常日之外，es也允许打开已经弃用的行为日志，默认这个日志是关闭的。可以通过config/loggin.yml文件中的日志级别设置成DEBUG来打开：1deprecation: DEBUG, deprecation_log_file 这将在日志记录下生成一个每天滚动的日志。定期检查这个文件，特鄙视准备升级到新版本的时候。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搜索引擎调研]]></title>
      <url>%2F2014%2F12%2F14%2Fsearch-engine-research%2F</url>
      <content type="text"><![CDATA[ElasticSeach简介ElasticSearch是一个实时的分布式搜索和分析引擎。它可以帮助你用前所未有的速度去处理大规模数据。它可以用于全文搜索，结构化搜索以及分析，当然你也可以将这三者进行组合。ElasticSearch是一个建立在全文搜索引擎Apache Lucene™基础上的搜索引擎，可以说Lucene是当今最先进，最高效的全功能开源搜索引擎框架。但是Lucene只是一个框架，要充分利用它的功能，需要使用Java，并且在程序中集成Lucene。需要很多的学习了解，才能明白它是如何运行的，Lucene确实非常复杂。Elasticsearch使用Lucene作为内部引擎，但是在使用它做全文搜索时，只需要使用统一开发好的API即可，而不需要了解其背后复杂的Lucene的运行原理。当然Elasticsearch并不仅仅是Lucene这么简单，它不但包括了全文搜索功能，还可以进行以下工作： 分布式实时文件存储，并将每一个字段都编入索引，使其可以被搜索。 实时分析的分布式搜索引擎。 可以拓展到上百台服务器，处理PB级别的结构化或非结构化数据。 这么多的功能被集成到一台服务器上，你可以轻松地通过客户端或者任何你喜欢的程序语言与ES的RESTful API进行交流。Elasticsearch的上手是非常简单的。它附带了很多非常合理的默认值，这让初学者很好地避免一上手就要面对复杂的理论，它安装好就可以使用了，用很小的学习成本就可以变得很有生产力。随着越学越深入，还可以利用Elasticsearch更多高级的功能，整个引擎可以很灵活地进行配置。可以根据自身需求来定制属于自己的Elasticsearch。使用案例： 维基百科使用Elasticsearch来进行全文搜索做高亮显示关键词，以及提供search-as-you-type、did-you-mean等搜索建议功能。 英国卫报使用Elasticsearch来处理访客日志，一遍能将公众对不同文章的反应实时地反馈给各位编辑。 StackOverflow将全文搜索与地理位置和相关信息进行结合，以提供more-like-this相关问题的展现。 GitHub使用Elasticsearch来检索超过1300亿行代码。 每天Goldman Sachs使用它来处理5TB数据的索引，还有很多投行使用它来分析股票市场的变动。 但是Elasticsearch并不只是面向大型企业的，它还帮助了很多类似DataDog以及Klout的创业公司进行了功能的拓展。 优缺点优点 Elasticsearch是分布式的。不需要其他组件，分布是实时的，被叫做“Push replication”. Elasticsearch完全支持Apache Lucene的接近实时的搜索。 处理多租户（multitenancy）不需要特殊配置，而Solr则需要更多的高级配置。 Elasticsearch采用Gateway的概念，使得备份更加简单。 各节点组成对等的网络结构，某些节点出现故障时会自动分配其他节点代替其进行工作。 缺点： 只有一名开发者（当前Elasticsearch GitHub组织已经不止如此，已经有了相当活跃的维护者） 还不够自动（不适合当前新的Index Warmup API） Solr简介Solr（读作“solar”）是Apache Lucene项目的开源企业搜索平台。其主要功能包括全文检索、命中标示、分面搜索、动态聚类、数据库集成，以及富文本（如Word、PDF）的处理。Solr是高度可拓展的，并提供了分布式搜索和索引复制。Solr是最流行的企业级搜索引擎，Solr4还增加了NoSQL支持。 Solr是用Java编写、运行在Servlet容器（如Apache Tomcat或Jetty）的一个独立的全文搜索服务器。Solr采用了Lucene Java搜索库为核心的全文索引和搜索，并具有类似REST的HTTP、XML和JSON的API。Solr强大的外部配置功能使得无需进行Java编码，即可对其进行调整以适应各种类型的应用程序。SOlr有一个插件架构，以支持更多的高级定制。 因为2010年Apache Lucene和Apache Solr项目合并，两个项目是由同一个Apache软件基金会开发团队制作实现的。提到技术或是产品时，Lucene/Solr或Solr/Lucene是一样的。 优缺点优点 Solr有一个更大、更成熟的用户、开发和贡献社区。 支持添加多种格式的索引，如：HTML、PDF、微软Office系列软件格式以及JSON、XML、CSV等纯文本格式。 Solr比较成熟、稳定。 不考虑建索引的同时进行搜索，速度更快。 缺点 建立索引时，搜索效率下降，实时索引搜索效率不高。 Elasticsearch与Solr的比较当单纯的对已有数据进行搜索时，Solr更快。 当实时建立索引时，Solr会产生io阻塞，查询性能较差，Elasticsearch具有明显的优势。 随着数据量的增加，Solr的搜索效率会变得更低，而Elasticsearch却没有明显的变化。 综上，Solr的架构不适合实时搜索的应用。 实际生产环境测试下图将搜索引擎从Solr转到Elasticsearch以后的平均查询速度有了50倍的提升。 总结 二者安装都很简单 Solr利用Zookeeper进行分布式管理，而Elasticsearch自身带有分布式协调管理功能 Solr支持更多的数据格式，而Elasticsearch仅支持Json数据格式 Solr官方提供的功能更多，而Elasticsearch本身更注重于核心功能，高级功能多有第三方插件提供 Solr在传统的搜索应用中表现好于Elasticsearch，但在处理实时搜索应用效率明显低于Elasticsearch Solr是传统搜索应用有力的解决方案，但Elasticsearch更适用于新兴的实时搜索应用。 其他基于Lucene的开源搜索引擎解决方案直接使用LuceneLucene是一个Java搜索类库，它本身并不是一个完整的解决方案，需要额外的开发工作。 优点成熟的解决方案，有很多成功案例。Apache顶级项目，正在持续快速的进步。庞大而活跃的开发社区，大量的开发人员。它只是一个类库，有足够的的定制和优化空间；经过简单定制，就可以满足绝大部分常见的需求；经过优化，可以支持10亿+量级的搜索。 缺点需要额外的开发工作。所有的拓展，分布式，可靠性等都需要自己实现；非实时，从建索引到可以搜索中间有一个时间延迟，而当前的“近实时”(Lucene Near Real Time search）搜索方案的可拓展性有待进一步完善 Katta基于Lucene的，支持分布式，可拓展，具有容错功能，准实时的搜索方案。 优点开箱即用，可以与Hadoop配合实现分布式。具备拓展和容错机制。 缺点只是搜索方案，建索引还是需要自己实现，在搜索功能上，只实现了最基本的需求。成功案例较少，项目的成熟度稍微差一些。因为需要支持分布式，对于一些复杂的查询需求，定制的难度会比较大。 Hadoop contrib/indexMap/Reduce模式的，分布式建索引方案，可以跟Katta配合使用。 优点分布式建索引，具备可扩展性。 缺点只是建索引方案，不包括搜索实现。工作在批处理模式，对实时搜索的支持不佳。 LinkedIn的开源方案基于Lucene的一系列解决方案，包括准实时搜索zoie，facet搜索实现bobo，机器学习算法decomposer，摘要存储krati，数据库模式包装sensei等等 优点经过验证的解决方案，支持分布式，可拓展，丰富的功能实现 缺点与LinkedIn公司联系太紧密，可定制性比较差 Lucandra基于Lucene，索引存在cassandra数据库中 优点参考cassandra的优点 缺点参考cassandra的缺点。另外，这只是一个demo，没有经过大量验证 HBasene基于Lucene，索引存在HBase数据库中 优点参考HBase的优点 缺点参考HBase的缺点。另外，在实现中，Lucene terms是存成行的，但每个term对应的posting lists是以列的方式存储的。随着单个term的postings lists增大，查询的速度受到的影响会非常大。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu--&quot;unmount:command not found&quot;的解决方法]]></title>
      <url>%2F2014%2F10%2F26%2Funmount-not-found%2F</url>
      <content type="text"><![CDATA[不得不说，这是个很弱的问题，今儿碰巧遇到。答案也非常简单。虽然没什么营养，但是为了加深印象，也为了碰到这个问题的其他人搜索起来不要那么“举目无亲”，还是记录下来。 根本没有什么unmount，而是umount。 错误命令：1sudo unmount /dev/sda3 正确命令：1sudo umount /dev/sda3 注：如果一定要使用unmount，可以设置别名alias unmount=&#39;umount&#39;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Go语言--并发篇]]></title>
      <url>%2F2014%2F09%2F13%2Fgo-goroutine%2F</url>
      <content type="text"><![CDATA[Go中并发程序依靠是：goroutine和channel 什么是goroutine？Goroutine，被普遍认为是协程的go语言实现。《Go语言编程》中说goroutine是轻量级线程(即协程coroutine, 原书90页). 在第九章进阶话题中, 作者又一次提到, “从根本上来说, goroutine就是一种go语言版本的协程(coroutine)” (原书204页). 但作者Rob Pike并不这么说。 “一个Goroutine是一个与其他goroutines并发运行在同一地址空间的Go函数或方法。一个运行的程序由一个或更多个goroutine组成。它与线程、协程、进程等不同。它是一个goroutine。” 1. goroutine vs 协程是go语言原生支持的，相对于一般由库实现协程的方式，goroutine更加强大，它的调度一定程度上是由go运行时（runtime）管理。其好处之一是，当某goroutine发生阻塞时（例如同步IO操作等），会自动出让CPU给其它goroutine。 2. goroutine vs 线程一个goroutine并不相当于一个线程，goroutine的出现正是为了替代原来的线程概念成为最小的调度范围，一旦运行goroutine时，先去当前线程查找，如果线程阻塞了，则被分配到空闲的线程，如果没有空闲的线程，那么就会新建一个线程。注意的是，当goroutine执行完毕后，线程不会回收退出，而是成为了空闲的线程。 goroutine的使用package main import ( &quot;fmt&quot; &quot;time&quot; ) func ready(w string, sec int64) { time.Sleep(time.Duration(sec * 1e9)) fmt.Println(w, &quot;is ready!&quot;) } func main() { go ready(&quot;Tee&quot;, 2) go ready(&quot;Coffee&quot;, 1) fmt.Println(&quot;I&apos;m waiting&quot;) time.Sleep(5 * 1e9) } 主线程为什么要sleep？ channel的使用channel是线程之间通信的管道 package main import ( &quot;fmt&quot; &quot;time&quot; ) var c chan int func ready(w string, sec int) { time.Sleep(int64(sec) * 1e9) fmt.Println(w, &quot;is ready!&quot;) c &lt;- 1 } func main() { c = make(chan int) go ready(&quot;Tee&quot;, 2) go ready(&quot;Coffee&quot;, 1) fmt.Println(&quot;I&apos;m waiting, but not too long&quot;) &lt;-c &lt;-c } channel 进一步理解channel分为两种：有buffer的，没有buffer的 默认的是没有buffer的 c1 := make(chan int)c2 := make(chan int, 0)c3 := make(chan int, 100) 有缓冲的channel，注意先“放”后“取”没有缓冲的channel，注意先“取”后“放” 图解并行编程1. 单个channel，单个goroutine, 一个写，一个读package main func main() { ch := make(chan int) go func() { ch &lt;- 42 }() &lt;-ch } 2. 加入定时器的实现package main import &quot;time&quot; func timer(d time.Duration) &lt;-chan int { c := make(chan int) go func() { time.Sleep(d) c &lt;- 1 }() return c } func main() { for i :=0; i &lt; 24; i++ { c := timer(i * time.Second) &lt;-c } } 3.乒乓模式import main import &quot;time&quot; func main() { var Ball int table := make(chan int) go player(table) go player(table) table &lt;- Ball time.Sleep(1 * time.Second) &lt;-table } func player(table chan int) { for { ball := &lt;-table ball++ time.Sleep(100 * time.Millisecond) table &lt;- ball } } 如果有3个人 go player(table) go player(table) go player(table) 如果有100个人 for i := 0: i&lt; 100; i++ { go player(table) } goroutines从某个特定的channel接收数据遵循FIFO order 4.Fan-In（扇入）并发世界的一个流行的编程模式————扇入模式。（相反是扇出模式）扇入是一个函数读取多个输入和多路复用到单通道。 note：在软件工程中，模块的扇入是指有多少个上级模块调用它。扇入越大，表示该模块被更多的上级模块共享。这当然是我们所希望的。但是不能为了获得高扇入而不惜代价，例如把彼此无关的功能凑在一起构成一个模块，虽然扇入数高了，但这样的模块内聚程度必然低。这是我们应避免的。 package main import ( &quot;fmt&quot; &quot;time&quot; ) func producer(ch chan int, d time.Duration) { var i int for { ch &lt;- i i++ time.Sleep(d) } } func reader(out chan int) { for x := range out { fmt.Println(x) } } func main() { ch := make(chan int) out := make(chan int) go producer(ch, 100*time.Millisecond) go producer(ch, 100*time.Millisecond) go reader(out) for i := range ch { out &lt;- i } } 5. Workers模式(fan-out) 扇出与扇入相反，多个函数从单一通道读取任务，分发任务到各个cpu核心上。 package main import ( &quot;fmt&quot; &quot;sync&quot; &quot;time&quot; ) func worker(taskCh &lt;-chan int, wg *sync.WaitGroup) { defer wg.Done() for { task, ok := &lt;-taskCh if !ok { return } d := time.Duration(task) * time.Millisecond time.Sleep(d) fmt.Println(&quot;processing task&quot;, task) } } func pool(wg *sync.WaitGroup, workers, tasks int) { tasksCh := make(chan int) for i := 0; i &lt; workers; i++ { go worker(taskCh, wg) } for i := 0; i &lt; tasks; i++ { taskCh &lt;- i } close(tasksCh) } func main() { var wg sync.WaitGroup wg.add(36) go poll(&amp;wg, 36, 50) wg.Wait() } 更复杂的例子： package main import ( &quot;fmt&quot; &quot;sync&quot; &quot;time&quot; ) const ( WORKERS = 5 SUBWORKERS = 3 TASKS = 20 SUBTASKS = 10 ) func subworker(subtasks chan int) { for { task, ok := &lt;-subtasks if !ok { return } time.Sleep(time.Duration(task) * time.Millisecond) fmt.Println(task) } } func worker(tasks &lt;-chan int, wg *sync.WaitGroup) { defer wg.Done() for { task, ok := &lt;-tasks if !ok { return } subtasks := make(chan int) for i := 0; i &lt; SUBWORKERS; i++ { go subworker(subtasks) } for i := 0; i &lt; SUBTASKS; i++ { task1 := task * i subtasks &lt;- task1 } close(subtasks) } } func main() { var wg sync.WaitGroup wg.Add(WORKERS) tasks := make(chan int) for i := 0; i &lt; WORKERS; i++ { go worker(tasks, &amp;wg) } for i := 0; i &lt; TASKS; i++ { tasks &lt;- i } close(tasks) wg.Wait() } servers模式 (和扇出相类似)package main import &quot;net&quot; func handler(c net.Conn) { c.Write([] byte(&quot;ok&quot;)) c.Close() } func main() { l, err := net.Listen(&quot;tcp&quot;, &quot;:5000&quot;) if err != nil { panic(err) } for { c, err := l.Accept() if err != nil { continue } go handler(c) } } Simplicity is complicated server模式2（logger）package main import ( &quot;fmt&quot; &quot;net&quot; &quot;time&quot; ) func handler(c net.Conn, ch chan string) { ch &lt;- c.RemoteAddr().String() c.Write([]byte(&quot;ok&quot;)) c.Close() } func logger(ch chan string) { for { fmt.Println(&lt;-ch) } } func server(l net.Listener, ch chan string) { for { c, err := l.Accept() if err != nil { continue } go handler(c, ch) } } func main() { l, err := net.Listen(&quot;tcp&quot;, &quot;:5000&quot;) if err != nil { paic(err) } ch := make(chan string) go logger(ch) go server(l, ch) time.Sleep(10 * time.Second) } Server + Worker 模式package main import ( &quot;net&quot; &quot;time&quot; ) func handler(c net.Conn, ch chan string) { addr := c.RemoteAddr().String() ch &lt;- addr time.Sleep(100 * time.Millisecond) c.Write([]byte(&quot;ok&quot;)) c.Close() } func logger(wch chan int, results chan int) { for { data := &lt;-wch data++ result &lt;- data } } func parse(results chan int) { for { &lt;-results } } func pool(ch chan string, n int) { wch := make(chan int) results := make(chan int) for i := 0; i &lt; n; i++ { go logger(wch, results) } go parse(results) for { addr := &lt;-ch l := len(addr) wch &lt;- l } } func server(l net.Listener, ch chan string) { for { c, err := l.Accept() if err != nil { continue } go handler(c, ch) } } func main() { l, err := net.Listen(&quot;tcp&quot;, &quot;:5000&quot;) if err != nil { panic(err) } ch := make(chan string) go pool(ch, 4) go server(l, ch) time.Sleep(10 * time.Second) } Concurrent Prime Sieve(素数帅选器)package main import &quot;fmt&quot; func Generate(ch chan&lt;- int) { for i := 2; ; i++ { ch &lt;- i } } func Filter(in &lt;-chan int, out chan&lt;- int, prime int) { for { i := &lt;-in if i % prime != 0 { out &lt;- i } } } func main() { ch := make(chan int) go Generate(ch) for i := 0; i &lt; 10; i++ { prime := &lt;-ch fmt.Println(prime) ch1 := make(chan int) go Filter(ch, ch1, prime) ch = ch1 } } Parallelism is not Concurrency简而言之: Parallelism is simply running things in parallel.Concurrency is a way to structure your program. 一图胜千言 parallelism 并行 concurrency 并发 Rob Pike-Concurrency Is Not Parallelism]]></content>
    </entry>

    
  
  
</search>
